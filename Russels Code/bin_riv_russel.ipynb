{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (Chapter 17)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define an MDP, and the special case of a GridMDP, in which states are laid out in a 2-dimensional grid. We also represent a policy\n",
    "as a dictionary of {state: action} pairs, and a Utility function as a dictionary of {state: number} pairs. We then define the value_iteration\n",
    "and policy_iteration algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from utils import vector_add, orientations, turn_right, turn_left"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [Page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "\n",
    "        self.init = init\n",
    "\n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state): # determins a numeric reward from this state\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001\n",
    "    def iterate_mdp(self, num_steps):\n",
    "        state = self.init\n",
    "        path = [state]\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            if state in self.terminals:\n",
    "                break\n",
    "\n",
    "            action = self.choose_action(state)\n",
    "            transitions = self.T(state, action)\n",
    "            probabilities, next_states = zip(*transitions)\n",
    "            state = random.choices(next_states, probabilities)[0]\n",
    "            path.append(state)\n",
    "\n",
    "        return path\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        return random.choice(self.actions(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example instantiation\n",
    "init_state = 'State 0'\n",
    "action_list = ['Action 0', 'Action 1']\n",
    "terminal_states = ['State 1']\n",
    "transitions = {\n",
    "    'State 0': {'Action 0': [(0.8, 'State 0'), (0.2, 'State 1')],\n",
    "                'Action 1': [(0.2, 'State 0'), (0.8, 'State 1')]},\n",
    "    'State 1': {'Action 0': [(0.8, 'State 0'), (0.2, 'State 1')],\n",
    "                'Action 1': [(0.2, 'State 0'), (0.8, 'State 1')]}\n",
    "}\n",
    "rewards = {'State 0': 0, 'State 1': 1}\n",
    "\n",
    "mdp = MDP(init_state, action_list, terminal_states, transitions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['State 0', 'State 0', 'State 1']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "init_state = 'State 0'\n",
    "action_list = ['Action 0', 'Action 1']\n",
    "terminal_states = ['State 1']\n",
    "transitions = {\n",
    "    'State 0': {'Action 0': [(0.8, 'State 0'), (0.2, 'State 1')],\n",
    "                'Action 1': [(0.2, 'State 0'), (0.8, 'State 1')]},\n",
    "    'State 1': {'Action 0': [(0.8, 'State 0'), (0.2, 'State 1')],\n",
    "                'Action 1': [(0.2, 'State 0'), (0.8, 'State 1')]}\n",
    "}\n",
    "rewards = {'State 0': 0, 'State 1': 1}\n",
    "\n",
    "mdp = MDP(init_state, action_list, terminal_states, transitions, rewards)\n",
    "path = mdp.iterate_mdp(10)\n",
    "print(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDP(MDP): #inherits from the MDP\n",
    "    \"\"\"A Partially Observable Markov Decision Process, defined by\n",
    "    a transition model P(s'|s,a), actions A(s), a reward function R(s),\n",
    "    and a sensor model P(e|s). We also keep track of a gamma value,\n",
    "    for use by algorithms. The transition and the sensor models\n",
    "    are defined as matrices. We also keep track of the possible states\n",
    "    and actions for each state. [Page 659].\"\"\"\n",
    "\n",
    "    def __init__(self, actions, transitions=None, evidences=None, rewards=None, states=None, gamma=0.95):\n",
    "        \"\"\"Initialize variables of the pomdp\"\"\"\n",
    "\n",
    "        #this is just to make sure that the gamma is between 0 and 1\n",
    "\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError('A POMDP must have 0 < gamma <= 1')\n",
    "\n",
    "        #states and actions\n",
    "\n",
    "        self.states = states \n",
    "        self.actions = actions\n",
    "\n",
    "        # transition model cannot be undefined\n",
    "\n",
    "        # our transition probability is defined as p_stay = 0.90\n",
    "\n",
    "        self.t_prob = transitions or {}\n",
    "        if not self.t_prob:\n",
    "            print('Warning: Transition model is undefined') #make sure that we have a transition probability\n",
    "\n",
    "        # sensor model cannot be undefined\n",
    "        self.e_prob = evidences or {}\n",
    "        if not self.e_prob:\n",
    "            print('Warning: Sensor model is undefined')\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def remove_dominated_plans(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans.\n",
    "        This method finds all the lines contributing to the\n",
    "        upper surface and removes those which don't.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = [values[0]]\n",
    "        y1_max = max(val[1] for val in values)\n",
    "        tgt = values[0]\n",
    "        prev_b = 0\n",
    "        prev_ix = 0\n",
    "        while tgt[1] != y1_max:\n",
    "            min_b = 1\n",
    "            min_ix = 0\n",
    "            for i in range(prev_ix + 1, len(values)):\n",
    "                if values[i][0] - tgt[0] + tgt[1] - values[i][1] != 0:\n",
    "                    trans_b = (values[i][0] - tgt[0]) / (values[i][0] - tgt[0] + tgt[1] - values[i][1])\n",
    "                    if 0 <= trans_b <= 1 and trans_b > prev_b and trans_b < min_b:\n",
    "                        min_b = trans_b\n",
    "                        min_ix = i\n",
    "            prev_b = min_b\n",
    "            prev_ix = min_ix\n",
    "            tgt = values[min_ix]\n",
    "            best.append(tgt)\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def remove_dominated_plans_fast(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans using approximations.\n",
    "        Resamples the upper boundary at intervals of 100 and\n",
    "        finds the maximum values at these points.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = []\n",
    "        sr = 100\n",
    "        for i in range(sr + 1):\n",
    "            x = i / float(sr)\n",
    "            maximum = (values[0][1] - values[0][0]) * x + values[0][0]\n",
    "            tgt = values[0]\n",
    "            for value in values:\n",
    "                val = (value[1] - value[0]) * x + value[0]\n",
    "                if val > maximum:\n",
    "                    maximum = val\n",
    "                    tgt = value\n",
    "\n",
    "            if all(any(tgt != v) for v in best):\n",
    "                best.append(np.array(tgt))\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def generate_mapping(self, best, input_values):\n",
    "        \"\"\"Generate mappings after removing dominated plans\"\"\"\n",
    "\n",
    "        mapping = defaultdict(list)\n",
    "        for value in best:\n",
    "            for action in input_values:\n",
    "                if any(all(value == v) for v in input_values[action]):\n",
    "                    mapping[action].append(value)\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def max_difference(self, U1, U2):\n",
    "        \"\"\"Find maximum difference between two utility mappings\"\"\"\n",
    "\n",
    "        for k, v in U1.items():\n",
    "            sum1 = 0\n",
    "            for element in U1[k]:\n",
    "                sum1 += sum(element)\n",
    "            sum2 = 0\n",
    "            for element in U2[k]:\n",
    "                sum2 += sum(element)\n",
    "        return abs(sum1 - sum2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition probability P(s'|s,a)\n",
    "t_prob = [[[0.9, 0.1], [0.1, 0.9]], [[0.1, 0.9], [0.9, 0.1]]] # find out how to do this for my case?\n",
    "# evidence function P(e|s) --> observation function\n",
    "e_prob = [[[0.6, 0.4], [0.4, 0.6]], [[0.6, 0.4], [0.4, 0.6]]] # find out how to do this in my case? --> obs values? but they have to be created \n",
    "# reward function\n",
    "rewards = [[0.0, 0.0], [1.0, 1.0]]\n",
    "# discount factor\n",
    "gamma = 0.95\n",
    "# actions\n",
    "actions = ('0', '1', '2')\n",
    "# states\n",
    "states = ('0', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp = POMDP(actions, t_prob, e_prob, rewards, states, gamma)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDP Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    \"\"\"Matrix operations class\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add(A, B):\n",
    "        \"\"\"Add two matrices A and B\"\"\"\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(A)):\n",
    "            row = []\n",
    "            for j in range(len(A[0])):\n",
    "                row.append(A[i][j] + B[i][j])\n",
    "            res.append(row)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def scalar_multiply(a, B):\n",
    "        \"\"\"Multiply scalar a to matrix B\"\"\"\n",
    "\n",
    "        for i in range(len(B)):\n",
    "            for j in range(len(B[0])):\n",
    "                B[i][j] = a * B[i][j]\n",
    "        return B\n",
    "\n",
    "    @staticmethod\n",
    "    def multiply(A, B):\n",
    "        \"\"\"Multiply two matrices A and B element-wise\"\"\"\n",
    "\n",
    "        matrix = []\n",
    "        for i in range(len(B)):\n",
    "            row = []\n",
    "            for j in range(len(B[0])):\n",
    "                row.append(B[i][j] * A[i][j])\n",
    "            matrix.append(row)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(A, B):\n",
    "        \"\"\"Inner-product of two matrices\"\"\"\n",
    "\n",
    "        return [[sum(ele_a * ele_b for ele_a, ele_b in zip(row_a, col_b)) for col_b in list(zip(*B))] for row_a in A]\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose(A):\n",
    "        \"\"\"Transpose a matrix\"\"\"\n",
    "\n",
    "        return [list(i) for i in zip(*A)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pomdp_value_iteration(pomdp, epsilon=0.1):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "\n",
    "    U = {'':[[0]* len(pomdp.states)]}\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        prev_U = U\n",
    "        values = [val for action in U for val in U[action]]\n",
    "        value_matxs = []\n",
    "        for i in values:\n",
    "            for j in values:\n",
    "                value_matxs.append([i, j])\n",
    "\n",
    "        U1 = defaultdict(list)\n",
    "        for action in pomdp.actions:\n",
    "            for u in value_matxs:\n",
    "                u1 = Matrix.matmul(Matrix.matmul(pomdp.t_prob[int(action)], Matrix.multiply(pomdp.e_prob[int(action)], Matrix.transpose(u))), [[1], [1]])\n",
    "                u1 = Matrix.add(Matrix.scalar_multiply(pomdp.gamma, Matrix.transpose(u1)), [pomdp.rewards[int(action)]])\n",
    "                U1[action].append(u1[0])\n",
    "\n",
    "        U = pomdp.remove_dominated_plans_fast(U1)\n",
    "        # replace with U = pomdp.remove_dominated_plans(U1) for accurate calculations\n",
    "        \n",
    "        if count > 10:\n",
    "            if pomdp.max_difference(U, prev_U) < epsilon * (1 - pomdp.gamma) / pomdp.gamma:\n",
    "                return U"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition function P(s'|s,a)\n",
    "t_prob = [[[0.65, 0.35], [0.65, 0.35]], [[0.65, 0.35], [0.65, 0.35]], [[1.0, 0.0], [0.0, 1.0]]]\n",
    "# evidence function P(e|s)\n",
    "e_prob = [[[0.5, 0.5], [0.5, 0.5]], [[0.5, 0.5], [0.5, 0.5]], [[0.8, 0.2], [0.3, 0.7]]] # create this also iteritevely --> is this belief or observation?\n",
    "# reward function\n",
    "rewards = [[5, -10], [-20, 5], [-1, -1]] #these have to depend on the reward function --> create a reward iteritevely\n",
    "\n",
    "gamma = 0.95\n",
    "actions = ('0', '1') #changing to two actions\n",
    "states = ('0', '1')\n",
    "\n",
    "pomdp = POMDP(actions, t_prob, e_prob, rewards, states, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Conditions\n",
    "p_stay = 0.89\n",
    "\n",
    "# transition probability --> prob of staying is p_stay \n",
    "t_prob = [[p_stay, 1-p_stay],[p_stay,1-p_stay]]\n",
    "\n",
    "# evidence function P(e|s) (still figuring this one out)\n",
    "e_prob = [[[0.6, 0.4], [0.4, 0.6]]]\n",
    "\n",
    "# for now fixed reward function for each state action pair\n",
    "rewards = [[1.5, 1.0], [2.0, 1.5]]\n",
    "\n",
    "# discount factor --> very low as for now we want to assume that immediate rewards are much more important than long-term rewards\n",
    "gamma = 0.05\n",
    "\n",
    "# actions\n",
    "actions = ('0', '1') #two actions \n",
    "\n",
    "# states\n",
    "states = ('0', '1') #two states\n",
    "\n",
    "pomdp = POMDP(actions, t_prob, e_prob, rewards, states, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m utility \u001b[39m=\u001b[39m pomdp_value_iteration(pomdp, epsilon\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mpomdp_value_iteration\u001b[0;34m(pomdp, epsilon)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m pomdp\u001b[39m.\u001b[39mactions:\n\u001b[1;32m     17\u001b[0m     \u001b[39mfor\u001b[39;00m u \u001b[39min\u001b[39;00m value_matxs:\n\u001b[0;32m---> 18\u001b[0m         u1 \u001b[39m=\u001b[39m Matrix\u001b[39m.\u001b[39mmatmul(Matrix\u001b[39m.\u001b[39;49mmatmul(pomdp\u001b[39m.\u001b[39;49mt_prob[\u001b[39mint\u001b[39;49m(action)], Matrix\u001b[39m.\u001b[39;49mmultiply(pomdp\u001b[39m.\u001b[39;49me_prob[\u001b[39mint\u001b[39;49m(action)], Matrix\u001b[39m.\u001b[39;49mtranspose(u))), [[\u001b[39m1\u001b[39m], [\u001b[39m1\u001b[39m]])\n\u001b[1;32m     19\u001b[0m         u1 \u001b[39m=\u001b[39m Matrix\u001b[39m.\u001b[39madd(Matrix\u001b[39m.\u001b[39mscalar_multiply(pomdp\u001b[39m.\u001b[39mgamma, Matrix\u001b[39m.\u001b[39mtranspose(u1)), [pomdp\u001b[39m.\u001b[39mrewards[\u001b[39mint\u001b[39m(action)]])\n\u001b[1;32m     20\u001b[0m         U1[action]\u001b[39m.\u001b[39mappend(u1[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m, in \u001b[0;36mMatrix.matmul\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatmul\u001b[39m(A, B):\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\"\"Inner-product of two matrices\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m [[\u001b[39msum\u001b[39m(ele_a \u001b[39m*\u001b[39m ele_b \u001b[39mfor\u001b[39;00m ele_a, ele_b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(row_a, col_b)) \u001b[39mfor\u001b[39;00m col_b \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mB))] \u001b[39mfor\u001b[39;00m row_a \u001b[39min\u001b[39;00m A]\n",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatmul\u001b[39m(A, B):\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\"\"Inner-product of two matrices\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m [[\u001b[39msum\u001b[39m(ele_a \u001b[39m*\u001b[39m ele_b \u001b[39mfor\u001b[39;00m ele_a, ele_b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(row_a, col_b)) \u001b[39mfor\u001b[39;00m col_b \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mB))] \u001b[39mfor\u001b[39;00m row_a \u001b[39min\u001b[39;00m A]\n",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatmul\u001b[39m(A, B):\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\"\"Inner-product of two matrices\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m [[\u001b[39msum\u001b[39m(ele_a \u001b[39m*\u001b[39m ele_b \u001b[39mfor\u001b[39;00m ele_a, ele_b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39;49m(row_a, col_b)) \u001b[39mfor\u001b[39;00m col_b \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mB))] \u001b[39mfor\u001b[39;00m row_a \u001b[39min\u001b[39;00m A]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "utility = pomdp_value_iteration(pomdp, epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnotebook\u001b[39;00m \u001b[39mimport\u001b[39;00m psource, pseudocode, plot_pomdp_utility\n\u001b[0;32m----> 4\u001b[0m plot_pomdp_utility(utility)\n",
      "File \u001b[0;32m~/Documents/Mod4NeuCog/M2 /Internship 3/POMDP_coding_project/codebase/Russels Code/notebook.py:1108\u001b[0m, in \u001b[0;36mplot_pomdp_utility\u001b[0;34m(utility)\u001b[0m\n\u001b[1;32m   1106\u001b[0m save \u001b[39m=\u001b[39m utility[\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m   1107\u001b[0m delete \u001b[39m=\u001b[39m utility[\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1108\u001b[0m ask_save \u001b[39m=\u001b[39m utility[\u001b[39m'\u001b[39;49m\u001b[39m2\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n\u001b[1;32m   1109\u001b[0m ask_delete \u001b[39m=\u001b[39m utility[\u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   1110\u001b[0m left \u001b[39m=\u001b[39m (save[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m ask_save[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m (save[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m ask_save[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m ask_save[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m save[\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from notebook import psource, pseudocode, plot_pomdp_utility\n",
    "plot_pomdp_utility(utility)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
