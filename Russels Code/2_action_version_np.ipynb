{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values: [array([0., 0.])]\n",
      "value_matxs: [array([[0., 0.],\n",
      "       [0., 0.]])]\n",
      "values: [array([3.5, 4. ]), array([3.5, 4. ])]\n",
      "value_matxs: [array([[3.5, 4. ],\n",
      "       [3.5, 4. ]]), array([[3.5, 4. ],\n",
      "       [3.5, 4. ]]), array([[3.5, 4. ],\n",
      "       [3.5, 4. ]]), array([[3.5, 4. ],\n",
      "       [3.5, 4. ]])]\n",
      "values: [array([6.87725, 7.74775]), array([6.87725, 7.74775])]\n",
      "value_matxs: [array([[6.87725, 7.74775],\n",
      "       [6.87725, 7.74775]]), array([[6.87725, 7.74775],\n",
      "       [6.87725, 7.74775]]), array([[6.87725, 7.74775],\n",
      "       [6.87725, 7.74775]]), array([[6.87725, 7.74775],\n",
      "       [6.87725, 7.74775]])]\n",
      "values: [array([10.12435475, 11.26939525]), array([10.12435475, 11.26939525])]\n",
      "value_matxs: [array([[10.12435475, 11.26939525],\n",
      "       [10.12435475, 11.26939525]]), array([[10.12435475, 11.26939525],\n",
      "       [10.12435475, 11.26939525]]), array([[10.12435475, 11.26939525],\n",
      "       [10.12435475, 11.26939525]]), array([[10.12435475, 11.26939525],\n",
      "       [10.12435475, 11.26939525]])]\n",
      "values: [array([13.23779374, 14.58626876]), array([13.23779374, 14.58626876])]\n",
      "value_matxs: [array([[13.23779374, 14.58626876],\n",
      "       [13.23779374, 14.58626876]]), array([[13.23779374, 14.58626876],\n",
      "       [13.23779374, 14.58626876]]), array([[13.23779374, 14.58626876],\n",
      "       [13.23779374, 14.58626876]]), array([[13.23779374, 14.58626876],\n",
      "       [13.23779374, 14.58626876]])]\n",
      "values: [array([16.2168197 , 17.71603968]), array([16.2168197 , 17.71603968])]\n",
      "value_matxs: [array([[16.2168197 , 17.71603968],\n",
      "       [16.2168197 , 17.71603968]]), array([[16.2168197 , 17.71603968],\n",
      "       [16.2168197 , 17.71603968]]), array([[16.2168197 , 17.71603968],\n",
      "       [16.2168197 , 17.71603968]]), array([[16.2168197 , 17.71603968],\n",
      "       [16.2168197 , 17.71603968]])]\n",
      "values: [array([19.0626472 , 20.67356921]), array([19.0626472 , 20.67356921])]\n",
      "value_matxs: [array([[19.0626472 , 20.67356921],\n",
      "       [19.0626472 , 20.67356921]]), array([[19.0626472 , 20.67356921],\n",
      "       [19.0626472 , 20.67356921]]), array([[19.0626472 , 20.67356921],\n",
      "       [19.0626472 , 20.67356921]]), array([[19.0626472 , 20.67356921],\n",
      "       [19.0626472 , 20.67356921]])]\n",
      "values: [array([21.77785619, 23.4715494 ]), array([21.77785619, 23.4715494 ])]\n",
      "value_matxs: [array([[21.77785619, 23.4715494 ],\n",
      "       [21.77785619, 23.4715494 ]]), array([[21.77785619, 23.4715494 ],\n",
      "       [21.77785619, 23.4715494 ]]), array([[21.77785619, 23.4715494 ],\n",
      "       [21.77785619, 23.4715494 ]]), array([[21.77785619, 23.4715494 ],\n",
      "       [21.77785619, 23.4715494 ]])]\n",
      "values: [array([24.36595432, 26.12098099]), array([24.36595432, 26.12098099])]\n",
      "value_matxs: [array([[24.36595432, 26.12098099],\n",
      "       [24.36595432, 26.12098099]]), array([[24.36595432, 26.12098099],\n",
      "       [24.36595432, 26.12098099]]), array([[24.36595432, 26.12098099],\n",
      "       [24.36595432, 26.12098099]]), array([[24.36595432, 26.12098099],\n",
      "       [24.36595432, 26.12098099]])]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from utils import vector_add, orientations, turn_right, turn_left\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [Page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states #or self.get_states_from_transitions(transitions)\n",
    "\n",
    "        self.init = init #is just the initial value which we can also just make randomly\n",
    "\n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions #or {}\n",
    "        # if not self.transitions:\n",
    "        #     print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward #or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state): #Reward Function\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "    \n",
    "    #this one needs to be extended \n",
    "\n",
    "    def T(self, state, action): #Transition Function\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "#this is just if the states were not passed but we'll make sure they are always passed\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001\n",
    "\n",
    "\n",
    "\n",
    "class POMDP(MDP):\n",
    "    \"\"\"A Partially Observable Markov Decision Process, defined by\n",
    "    a transition model P(s'|s,a), actions A(s), a reward function R(s),\n",
    "    and a sensor model P(e|s). We also keep track of a gamma value,\n",
    "    for use by algorithms. The transition and the sensor models\n",
    "    are defined as matrices. We also keep track of the possible states\n",
    "    and actions for each state. [Page 659].\"\"\"\n",
    "\n",
    "    def __init__(self, actions, transitions=None, evidences=None, rewards=None, states=None, gamma=0.95):\n",
    "        \"\"\"Initialize variables of the pomdp\"\"\"\n",
    "\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError('A POMDP must have 0 < gamma <= 1')\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # transition model cannot be undefined\n",
    "        self.t_prob = transitions or {}\n",
    "        if not self.t_prob:\n",
    "            print('Warning: Transition model is undefined')\n",
    "\n",
    "        # sensor model cannot be undefined\n",
    "        self.e_prob = evidences or {}\n",
    "        if not self.e_prob:\n",
    "            print('Warning: Sensor model is undefined')\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def remove_dominated_plans(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans.\n",
    "        This method finds all the lines contributing to the\n",
    "        upper surface and removes those which don't.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = [values[0]]\n",
    "        y1_max = max(val[1] for val in values)\n",
    "        tgt = values[0]\n",
    "        prev_b = 0\n",
    "        prev_ix = 0\n",
    "        while tgt[1] != y1_max:\n",
    "            min_b = 1\n",
    "            min_ix = 0\n",
    "            for i in range(prev_ix + 1, len(values)):\n",
    "                if values[i][0] - tgt[0] + tgt[1] - values[i][1] != 0:\n",
    "                    trans_b = (values[i][0] - tgt[0]) / (values[i][0] - tgt[0] + tgt[1] - values[i][1])\n",
    "                    if 0 <= trans_b <= 1 and trans_b > prev_b and trans_b < min_b:\n",
    "                        min_b = trans_b\n",
    "                        min_ix = i\n",
    "            prev_b = min_b\n",
    "            prev_ix = min_ix\n",
    "            tgt = values[min_ix]\n",
    "            best.append(tgt)\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def remove_dominated_plans_fast(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans using approximations.\n",
    "        Resamples the upper boundary at intervals of 100 and\n",
    "        finds the maximum values at these points.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = []\n",
    "        sr = 100\n",
    "        for i in range(sr + 1):\n",
    "            x = i / float(sr)\n",
    "            maximum = (values[0][1] - values[0][0]) * x + values[0][0]\n",
    "            tgt = values[0]\n",
    "            for value in values:\n",
    "                val = (value[1] - value[0]) * x + value[0]\n",
    "                if val > maximum:\n",
    "                    maximum = val\n",
    "                    tgt = value\n",
    "\n",
    "            if all(any(tgt != v) for v in best):\n",
    "                best.append(np.array(tgt))\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def generate_mapping(self, best, input_values):\n",
    "        \"\"\"Generate mappings after removing dominated plans\"\"\"\n",
    "\n",
    "        mapping = defaultdict(list)\n",
    "        for value in best:\n",
    "            for action in input_values:\n",
    "                if any(all(value == v) for v in input_values[action]):\n",
    "                    mapping[action].append(value)\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def max_difference(self, U1, U2):\n",
    "        \"\"\"Find maximum difference between two utility mappings\"\"\"\n",
    "\n",
    "        for k, v in U1.items():\n",
    "            sum1 = 0\n",
    "            for element in U1[k]:\n",
    "                sum1 += sum(element)\n",
    "            sum2 = 0\n",
    "            for element in U2[k]:\n",
    "                sum2 += sum(element)\n",
    "        return abs(sum1 - sum2)\n",
    "\n",
    "#%%\n",
    "\n",
    "class Matrix:\n",
    "    \"\"\"Matrix operations class\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add(A, B):\n",
    "        \"\"\"Add two matrices A and B\"\"\"\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(A)):\n",
    "            row = []\n",
    "            for j in range(len(A[0])):\n",
    "                row.append(A[i][j] + B[i][j])\n",
    "            res.append(row)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def scalar_multiply(a, B):\n",
    "        \"\"\"Multiply scalar a to matrix B\"\"\"\n",
    "\n",
    "        for i in range(len(B)):\n",
    "            for j in range(len(B[0])):\n",
    "                B[i][j] = a * B[i][j]\n",
    "        return B\n",
    "\n",
    "    @staticmethod\n",
    "    def multiply(A, B):\n",
    "        \"\"\"Multiply two matrices A and B element-wise\"\"\"\n",
    "\n",
    "        matrix = []\n",
    "        for i in range(len(B)):\n",
    "            row = []\n",
    "            for j in range(len(B[0])):\n",
    "                row.append(B[i][j] * A[j][i])\n",
    "            matrix.append(row)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(A, B):\n",
    "        \"\"\"Inner-product of two matrices\"\"\"\n",
    "\n",
    "        return [[sum(ele_a * ele_b for ele_a, ele_b in zip(row_a, col_b)) for col_b in list(zip(*B))] for row_a in A]\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose(A):\n",
    "        \"\"\"Transpose a matrix\"\"\"\n",
    "\n",
    "        return [list(i) for i in zip(*A)]\n",
    "    \n",
    "def pomdp_value_iteration(pomdp, epsilon=0.1):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "\n",
    "    U = {'':[[0]* len(pomdp.states)]}\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        prev_U = U\n",
    "        values = [val for action in U for val in U[action]]\n",
    "        value_matxs = []\n",
    "        for i in values:\n",
    "            for j in values:\n",
    "                value_matxs.append([i, j])\n",
    "\n",
    "        U1 = defaultdict(list)\n",
    "        for action in pomdp.actions:\n",
    "            for u in value_matxs:\n",
    "                u1 = Matrix.matmul(Matrix.matmul(pomdp.t_prob[int(action)], Matrix.multiply(pomdp.e_prob[int(action)], Matrix.transpose(u))), [[1], [1]])\n",
    "                u1 = Matrix.add(Matrix.scalar_multiply(pomdp.gamma, Matrix.transpose(u1)), [pomdp.rewards[int(action)]])\n",
    "                U1[action].append(u1[0])\n",
    "\n",
    "        U = pomdp.remove_dominated_plans_fast(U1)\n",
    "        # replace with U = pomdp.remove_dominated_plans(U1) for accurate calculations\n",
    "        \n",
    "        if count > 10:\n",
    "            if pomdp.max_difference(U, prev_U) < epsilon * (1 - pomdp.gamma) / pomdp.gamma:\n",
    "                return U\n",
    "\n",
    "def pomdp_value_iteration_numpy(pomdp, epsilon=0.1):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "\n",
    "    U = {'': np.zeros((1, len(pomdp.states)))}\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        prev_U = U\n",
    "        values = [val for action in U for val in U[action]]\n",
    "        value_matxs = []\n",
    "        for i in values:\n",
    "            for j in values:\n",
    "                value_matxs.append(np.array([i, j]))\n",
    "\n",
    "        if count < 10:\n",
    "            print(f\"values: {values}\")\n",
    "            print(f\"value_matxs: {value_matxs}\")\n",
    "        U1 = defaultdict(list)\n",
    "        for action in pomdp.actions:\n",
    "            for u in value_matxs:\n",
    "                u1 = np.matmul(np.matmul(pomdp.t_prob[int(action)], np.multiply(pomdp.e_prob[int(action)], u.T)), np.array([[1], [1]]))\n",
    "                u1 = np.add(np.multiply(pomdp.gamma, u1.T), [pomdp.rewards[int(action)]])\n",
    "                U1[action].append(u1[0])\n",
    "\n",
    "        U = pomdp.remove_dominated_plans_fast(U1)\n",
    "        # replace with U = pomdp.remove_dominated_plans(U1) for accurate calculations\n",
    "        \n",
    "        if count > 10:\n",
    "            if pomdp.max_difference(U, prev_U) < epsilon * (1 - pomdp.gamma) / pomdp.gamma:\n",
    "                return U\n",
    "\n",
    "p_stay = 0.89\n",
    "# # Initial Conditions\n",
    "\n",
    "# transition probability P(s'|s,a)\n",
    "t_prob = [[[p_stay, 1-p_stay], [1-p_stay,p_stay]], [[p_stay, 1-p_stay], [1-p_stay,p_stay]]]\n",
    "# evidence function P(e|s)\n",
    "e_prob = [[[0.6, 0.4], [0.4, 0.6]], [[0.6, 0.4], [0.4, 0.6]]]\n",
    "# reward function\n",
    "rewards = [[3.5, 4.0], [3.5, 4.0]]\n",
    "# discount factor\n",
    "gamma = 0.95\n",
    "# actions\n",
    "actions = ('0', '1') #stay and switch\n",
    "# states\n",
    "states = ('0', '1')\n",
    "\n",
    "pomdp = POMDP(actions, t_prob, e_prob, rewards, states, gamma)\n",
    "\n",
    "utility = pomdp_value_iteration_numpy(pomdp, epsilon=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[array([73.83931532, 75.76981726])]\n",
      "[array([73.83931532, 75.76981726])]\n"
     ]
    }
   ],
   "source": [
    "print(len(utility['1']))\n",
    "print((utility['1']))\n",
    "print((utility['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pomdp_utility(utility):\n",
    "    # att_left = utility['0'][0]\n",
    "    # att_right = utility['1'][0]\n",
    "\n",
    "    # left = att_left[0] / (att_left[0] + att_right[0])\n",
    "    # right = att_right[0] / (att_left[0] + att_right[0])\n",
    "\n",
    "    colors = ['g', 'b']\n",
    "    for action in utility:\n",
    "        for value in utility[action]:\n",
    "            print(value)\n",
    "            plt.plot(value, color=colors[int(action)])\n",
    "\n",
    "\n",
    "    # plt.vlines([left, right], -20, 10, linestyles='dashed', colors='c')\n",
    "    plt.ylim(-20, 13)\n",
    "    plt.xlim(0, 1)\n",
    "\n",
    "    # plt.text(left / 2 - 0.05, 10, 'att_left')\n",
    "    # plt.text((right + left) / 2 - 0.02, 10, 'Ask')\n",
    "    # plt.text((right + 1) / 2 - 0.07, 10, 'att_right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73.83931532 75.76981726]\n",
      "[73.83931532 75.76981726]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGdCAYAAAABhTmFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbLklEQVR4nO3de2zV9f348VehcChoj5e6tki5eJu3zQtMKMYom6LOacyik7kYXJRpHPGCzEgwCu6LjaJm8z6dU5foNNNptqgTtjDEuzBINnHxAq44rAzQHkR3UPj8/jD0Z9fqKPb0tG8ej+ST7HzO55y+ynvN5+nnnNNWZFmWBQBAAvqVewAAgO4ibACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEhGZbkH+LK2bNkSq1evjp133jkqKirKPQ4AsA2yLIsNGzbE0KFDo1+/7rvO0ufDZvXq1dHQ0FDuMQCA7bBq1aoYNmxYtz1fnw+bnXfeOSI+/Yeprq4u8zQAwLYoFArR0NDQdh7vLn0+bLa+/FRdXS1sAKCP6e63kXjzMACQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACSjpGHz9NNPx8knnxxDhw6NioqKeOyxx9rdn2VZzJo1K4YOHRpVVVVxzDHHxCuvvFLKkQCAhJU0bDZu3BiHHHJI3HLLLZ3ef91118WNN94Yt9xyS7z88stRV1cXxx13XGzYsKGUYwEAiaos5ZOfeOKJceKJJ3Z6X5Zl8bOf/SxmzpwZ3/3udyMi4r777ova2tp44IEH4rzzzivlaABAgsr2HpuVK1dGS0tLTJw4sW1fLpeLo48+Op577rnPfVyxWIxCodBuAwCIKGPYtLS0REREbW1tu/21tbVt93Wmqakp8vl829bQ0FDSOQGAvqPsn4qqqKhodzvLsg77PmvGjBnR2tratq1atarUIwIAfURJ32PzRerq6iLi0ys39fX1bfvXrFnT4SrOZ+VyucjlciWfDwDoe8p2xWbUqFFRV1cX8+fPb9u3adOmWLhwYYwfP75cYwEAfVhJr9h88MEH8cYbb7TdXrlyZSxbtix22223GD58eFx88cVxzTXXxL777hv77rtvXHPNNTF48OA488wzSzkWAJCokobN4sWLY8KECW23p02bFhERkydPjnvvvTcuu+yy+Oijj+KCCy6I9957L8aOHRvz5s2LnXfeuZRjAQCJqsiyLCv3EF9GoVCIfD4fra2tUV1dXe5xAIBtUKrzd9k/FQUA0F2EDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMsoeNrNmzYqKiop2W11dXbnHAgD6oMpyDxARcdBBB8Wf/vSnttv9+/cv4zQAQF/VK8KmsrLSVRoA4Esr+0tRERGvv/56DB06NEaNGhWTJk2KFStWfO6xxWIxCoVCuw0AIKIXhM3YsWPj17/+dTz11FNx1113RUtLS4wfPz7WrVvX6fFNTU2Rz+fbtoaGhh6eGADorSqyLMvKPcRnbdy4Mfbee++47LLLYtq0aR3uLxaLUSwW224XCoVoaGiI1tbWqK6u7slRAYDtVCgUIp/Pd/v5u1e8x+azhgwZEl/72tfi9ddf7/T+XC4XuVyuh6cCAPqCsr8U9d+KxWK8+uqrUV9fX+5RAIA+puxhM3369Fi4cGGsXLkyXnzxxTjttNOiUCjE5MmTyz0aANDHlP2lqLfffju+//3vx9q1a2OPPfaIcePGxQsvvBAjRowo92gAQB9T9rB58MEHyz0CAJCIsr8UBQDQXYQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyekXY3HbbbTFq1KgYNGhQjB49OhYtWlTukQCAPqjsYfPQQw/FxRdfHDNnzoylS5fGUUcdFSeeeGI0NzeXezQAoI+pyLIsK+cAY8eOjcMPPzxuv/32tn0HHHBAnHrqqdHU1PQ/H18oFCKfz0dra2tUV1eXclQAoJuU6vxd1is2mzZtiiVLlsTEiRPb7Z84cWI899xznT6mWCxGoVBotwEARJQ5bNauXRubN2+O2tradvtra2ujpaWl08c0NTVFPp9v2xoaGnpiVACgDyj7e2wiIioqKtrdzrKsw76tZsyYEa2trW3bqlWremJEAKAPqCznF6+pqYn+/ft3uDqzZs2aDldxtsrlcpHL5XpiPACgjynrFZuBAwfG6NGjY/78+e32z58/P8aPH1+mqQCAvqqsV2wiIqZNmxZnnXVWjBkzJhobG+POO++M5ubmOP/888s9GgDQx5Q9bM4444xYt25dXH311fHOO+/EwQcfHE888USMGDGi3KMBAH1M2X+PzZfl99gAQN+T5O+xAQDoTsIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZZQ2bkSNHRkVFRbvt8ssvL+dIAEAfVlnuAa6++uqYMmVK2+2ddtqpjNMAAH1Z2cNm5513jrq6unKPAQAkoOzvsbn22mtj9913j0MPPTTmzJkTmzZt+sLji8ViFAqFdhsAQESZr9hcdNFFcfjhh8euu+4aL730UsyYMSNWrlwZv/zlLz/3MU1NTTF79uwenBIA6CsqsizLuvMJZ82a9T/D4+WXX44xY8Z02P/II4/EaaedFmvXro3dd9+908cWi8UoFotttwuFQjQ0NERra2tUV1d/ueEBgB5RKBQin893+/m726/YTJ06NSZNmvSFx4wcObLT/ePGjYuIiDfeeONzwyaXy0Uul/tSMwIAaer2sKmpqYmamprteuzSpUsjIqK+vr47RwIAdhBle4/N888/Hy+88EJMmDAh8vl8vPzyy3HJJZfEKaecEsOHDy/XWABAH1a2sMnlcvHQQw/F7Nmzo1gsxogRI2LKlClx2WWXlWskAKCPK1vYHH744fHCCy+U68sDAAkq+++xAQDoLsIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZJQ2bOXPmxPjx42Pw4MGxyy67dHpMc3NznHzyyTFkyJCoqamJCy+8MDZt2lTKsQCARFWW8sk3bdoUp59+ejQ2Nsbdd9/d4f7NmzfHSSedFHvssUc888wzsW7dupg8eXJkWRY333xzKUcDABJU0rCZPXt2RETce++9nd4/b968WL58eaxatSqGDh0aERE33HBDnH322TFnzpyorq4u5XgAQGLK+h6b559/Pg4++OC2qImIOP7446NYLMaSJUs6fUyxWIxCodBuAwCIKHPYtLS0RG1tbbt9u+66awwcODBaWlo6fUxTU1Pk8/m2raGhoSdGBQD6gC6HzaxZs6KiouILt8WLF2/z81VUVHTYl2VZp/sjImbMmBGtra1t26pVq7r6LQAAierye2ymTp0akyZN+sJjRo4cuU3PVVdXFy+++GK7fe+99158/PHHHa7kbJXL5SKXy23T8wMAO5Yuh01NTU3U1NR0yxdvbGyMOXPmxDvvvBP19fUR8ekbinO5XIwePbpbvgYAsOMo6aeimpubY/369dHc3BybN2+OZcuWRUTEPvvsEzvttFNMnDgxDjzwwDjrrLNi7ty5sX79+pg+fXpMmTLFJ6IAgC4radhceeWVcd9997XdPuywwyIiYsGCBXHMMcdE//794/HHH48LLrggjjzyyKiqqoozzzwzrr/++lKOBQAkqiLLsqzcQ3wZhUIh8vl8tLa2usoDAH1Eqc7f/lYUAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkAxhAwAkQ9gAAMkQNgBAMoQNAJAMYQMAJEPYAADJEDYAQDKEDQCQDGEDACRD2AAAyRA2AEAyhA0AkIyShs2cOXNi/PjxMXjw4Nhll106PaaioqLDdscdd5RyLAAgUZWlfPJNmzbF6aefHo2NjXH33Xd/7nH33HNPnHDCCW238/l8KccCABJV0rCZPXt2RETce++9X3jcLrvsEnV1daUcBQDYAfSK99hMnTo1ampq4hvf+EbccccdsWXLls89tlgsRqFQaLcBAESU+IrNtvjpT38a3/rWt6Kqqir+/Oc/x6WXXhpr166NK664otPjm5qa2q4EAQB8Vpev2MyaNavTN/x+dlu8ePE2P98VV1wRjY2Nceihh8all14aV199dcydO/dzj58xY0a0tra2batWrerqtwAAJKrLV2ymTp0akyZN+sJjRo4cub3zxLhx46JQKMS7774btbW1He7P5XKRy+W2+/kBgHR1OWxqamqipqamFLNERMTSpUtj0KBBn/vxcACAz1PS99g0NzfH+vXro7m5OTZv3hzLli2LiIh99tkndtppp/jDH/4QLS0t0djYGFVVVbFgwYKYOXNm/OhHP3JVBgDospKGzZVXXhn33Xdf2+3DDjssIiIWLFgQxxxzTAwYMCBuu+22mDZtWmzZsiX22muvuPrqq+PHP/5xKccCABJVkWVZVu4hvoxCoRD5fD5aW1ujurq63OMAANugVOfvXvF7bAAAuoOwAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJIhbACAZAgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIRsnC5q233opzzjknRo0aFVVVVbH33nvHVVddFZs2bWp3XHNzc5x88skxZMiQqKmpiQsvvLDDMQAA26KyVE/8j3/8I7Zs2RK/+MUvYp999om///3vMWXKlNi4cWNcf/31ERGxefPmOOmkk2KPPfaIZ555JtatWxeTJ0+OLMvi5ptvLtVoAECiKrIsy3rqi82dOzduv/32WLFiRUREPPnkk/Gd73wnVq1aFUOHDo2IiAcffDDOPvvsWLNmTVRXV//P5ywUCpHP56O1tXWbjgcAyq9U5++SXbHpTGtra+y2225tt59//vk4+OCD26ImIuL444+PYrEYS5YsiQkTJnR4jmKxGMVisd1zRnz6DwQA9A1bz9vdfX2lx8LmzTffjJtvvjluuOGGtn0tLS1RW1vb7rhdd901Bg4cGC0tLZ0+T1NTU8yePbvD/oaGhu4dGAAouXXr1kU+n++25+ty2MyaNavTsPisl19+OcaMGdN2e/Xq1XHCCSfE6aefHueee267YysqKjo8PsuyTvdHRMyYMSOmTZvWdvv999+PESNGRHNzc7f+w9B1hUIhGhoaYtWqVV4WLDNr0btYj97DWvQera2tMXz48Hav5HSHLofN1KlTY9KkSV94zMiRI9v+9+rVq2PChAnR2NgYd955Z7vj6urq4sUXX2y377333ouPP/64w5WcrXK5XORyuQ778/m8/5P2EtXV1dail7AWvYv16D2sRe/Rr1/3fkC7y2FTU1MTNTU123Tsv/71r5gwYUKMHj067rnnng7DNzY2xpw5c+Kdd96J+vr6iIiYN29e5HK5GD16dFdHAwB2cCV7j83q1avjmGOOieHDh8f1118f//73v9vuq6uri4iIiRMnxoEHHhhnnXVWzJ07N9avXx/Tp0+PKVOmKGkAoMtKFjbz5s2LN954I954440YNmxYu/u2vgO6f//+8fjjj8cFF1wQRx55ZFRVVcWZZ57Z9ntutkUul4urrrqq05en6FnWovewFr2L9eg9rEXvUaq16NHfYwMAUEr+VhQAkAxhAwAkQ9gAAMkQNgBAMvpE2Nx2220xatSoGDRoUIwePToWLVr0hccvXLgwRo8eHYMGDYq99tor7rjjjh6aNH1dWYvf/e53cdxxx8Uee+wR1dXV0djYGE899VQPTpu2rv5cbPXss89GZWVlHHrooaUdcAfT1fUoFosxc+bMGDFiRORyudh7773jV7/6VQ9Nm7aursX9998fhxxySAwePDjq6+vjhz/8Yaxbt66Hpk3X008/HSeffHIMHTo0Kioq4rHHHvufj+mW83fWyz344IPZgAEDsrvuuitbvnx5dtFFF2VDhgzJ/vnPf3Z6/IoVK7LBgwdnF110UbZ8+fLsrrvuygYMGJA9/PDDPTx5erq6FhdddFF27bXXZi+99FL22muvZTNmzMgGDBiQ/fWvf+3hydPT1bXY6v3338/22muvbOLEidkhhxzSM8PuALZnPU455ZRs7Nix2fz587OVK1dmL774Yvbss8/24NRp6upaLFq0KOvXr1/285//PFuxYkW2aNGi7KCDDspOPfXUHp48PU888UQ2c+bM7JFHHskiInv00Ue/8PjuOn/3+rA54ogjsvPPP7/dvv333z+7/PLLOz3+sssuy/bff/92+84777xs3LhxJZtxR9HVtejMgQcemM2ePbu7R9vhbO9anHHGGdkVV1yRXXXVVcKmG3V1PZ588sksn89n69at64nxdihdXYu5c+dme+21V7t9N910UzZs2LCSzbgj2paw6a7zd69+KWrTpk2xZMmSmDhxYrv9EydOjOeee67Txzz//PMdjj/++ONj8eLF8fHHH5ds1tRtz1r8ty1btsSGDRu6/Q+e7Wi2dy3uueeeePPNN+Oqq64q9Yg7lO1Zj9///vcxZsyYuO6662LPPfeM/fbbL6ZPnx4fffRRT4ycrO1Zi/Hjx8fbb78dTzzxRGRZFu+++248/PDDcdJJJ/XEyHxGd52/S/abh7vD2rVrY/PmzR3+IGZtbW20tLR0+piWlpZOj//kk09i7dq1bX+Tiq7ZnrX4bzfccENs3Lgxvve975VixB3G9qzF66+/HpdffnksWrQoKit79Y99n7M967FixYp45plnYtCgQfHoo4/G2rVr44ILLoj169d7n82XsD1rMX78+Lj//vvjjDPOiP/85z/xySefxCmnnBI333xzT4zMZ3TX+btXX7HZqqKiot3tLMs67Ptfx3e2n67r6lps9Zvf/CZmzZoVDz30UHzlK18p1Xg7lG1di82bN8eZZ54Zs2fPjv3226+nxtvhdOVnY8uWLVFRURH3339/HHHEEfHtb387brzxxrj33ntdtekGXVmL5cuXx4UXXhhXXnllLFmyJP74xz/GypUr4/zzz++JUfkv3XH+7tX/6VZTUxP9+/fvUNpr1qzpUHVb1dXVdXp8ZWVl7L777iWbNXXbsxZbPfTQQ3HOOefEb3/72zj22GNLOeYOoatrsWHDhli8eHEsXbo0pk6dGhGfnlizLIvKysqYN29efPOb3+yR2VO0PT8b9fX1seeee0Y+n2/bd8ABB0SWZfH222/HvvvuW9KZU7U9a9HU1BRHHnlk/OQnP4mIiK9//esxZMiQOOqoo+L//u//XOXvQd11/u7VV2wGDhwYo0ePjvnz57fbP3/+/Bg/fnynj2lsbOxw/Lx582LMmDExYMCAks2auu1Zi4hPr9ScffbZ8cADD3jNupt0dS2qq6vjb3/7WyxbtqxtO//88+OrX/1qLFu2LMaOHdtToydpe342jjzyyFi9enV88MEHbftee+216NevX4c/Gsy22561+PDDD6Nfv/anwv79+0fE/79aQM/otvN3l95qXAZbP7p39913Z8uXL88uvvjibMiQIdlbb72VZVmWXX755dlZZ53VdvzWj4tdcskl2fLly7O7777bx727SVfX4oEHHsgqKyuzW2+9NXvnnXfatvfff79c30IyuroW/82norpXV9djw4YN2bBhw7LTTjste+WVV7KFCxdm++67b3buueeW61tIRlfX4p577skqKyuz2267LXvzzTezZ555JhszZkx2xBFHlOtbSMaGDRuypUuXZkuXLs0iIrvxxhuzpUuXtn30vlTn714fNlmWZbfeems2YsSIbODAgdnhhx+eLVy4sO2+yZMnZ0cffXS74//yl79khx12WDZw4MBs5MiR2e23397DE6erK2tx9NFHZxHRYZs8eXLPD56grv5cfJaw6X5dXY9XX301O/bYY7Oqqqps2LBh2bRp07IPP/ywh6dOU1fX4qabbsoOPPDArKqqKquvr89+8IMfZG+//XYPT52eBQsWfOE5oFTn74osc60NAEhDr36PDQBAVwgbACAZwgYASIawAQCSIWwAgGQIGwAgGcIGAEiGsAEAkiFsAIBkCBsAIBnCBgBIhrABAJLx/wA0RJ72ix76YwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pomdp_utility(utility)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
